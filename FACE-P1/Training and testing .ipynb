{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5e7f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Quadro RTX 3000, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Feb 17 13:11:12 2023\n",
    "\n",
    "@author: Debra Hogue\n",
    "\n",
    "Modified RankNet by Lv et al. to use Tensorflow not Pytorch\n",
    "and added additional comments to explain methods\n",
    "\n",
    "Paper: Simultaneously Localize, Segment and Rank the Camouflaged Objects by Lv et al.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, layers, losses\n",
    "import numpy as np\n",
    "import os, argparse\n",
    "from datetime import datetime\n",
    "from Attention.ResNet_models import Generator\n",
    "from data import get_loader\n",
    "from utils import adjust_lr, AvgMeter\n",
    "from scipy import misc\n",
    "import cv2\n",
    "from data import test_dataset\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2\n",
    "import tensorflow.keras.applications.resnet50 as models # instantiates the ResNet50 architecture \n",
    "\n",
    "from utils import l2_regularisation\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=5632)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "tf.config.optimizer.set_experimental_options({\"Memory optimizer\": True, \"Layout optimizer\": True})\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203e9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--epoch', type=int, default=1, help='epoch number')\n",
    "# parser.add_argument('--lr_gen', type=float, default=2.5e-5, help='learning rate for generator')\n",
    "# parser.add_argument('--batchsize', type=int, default=2, help='training batch size')\n",
    "# parser.add_argument('--trainsize', type=int, default=480, help='training dataset size')\n",
    "# parser.add_argument('--decay_rate', type=float, default=0.9, help='decay rate of learning rate')\n",
    "# parser.add_argument('--decay_epoch', type=int, default=40, help='every n epochs decay learning rate')\n",
    "# parser.add_argument('--feat_channel', type=int, default=32, help='reduced channel of saliency feat')\n",
    "# opt = parser.parse_args()\n",
    "# print('Generator Learning Rate: {}'.format(opt.lr_gen))\n",
    "epoch = 80\n",
    "decay_rate = 0.97\n",
    "decay_epoch = 65\n",
    "batchsize = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744bfe3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "\n",
      "Epoch: 0. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 1/80\n",
      "2001/2001 [==============================] - 961s 463ms/step - loss: 1.8245 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 1. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 2/80\n",
      "2001/2001 [==============================] - 905s 452ms/step - loss: 1.5036 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 2. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 3/80\n",
      "2001/2001 [==============================] - 900s 450ms/step - loss: 1.3275 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 3. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 4/80\n",
      "2001/2001 [==============================] - 898s 449ms/step - loss: 1.2216 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 4. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 5/80\n",
      "2001/2001 [==============================] - 902s 451ms/step - loss: 1.1441 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 5. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 6/80\n",
      "2001/2001 [==============================] - 886s 443ms/step - loss: 1.0836 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 6. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 7/80\n",
      "2001/2001 [==============================] - 887s 443ms/step - loss: 1.0440 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 7. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 8/80\n",
      "2001/2001 [==============================] - 903s 451ms/step - loss: 0.9979 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 8. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 9/80\n",
      "2001/2001 [==============================] - 885s 442ms/step - loss: 0.9617 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 9. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 10/80\n",
      "2001/2001 [==============================] - 892s 446ms/step - loss: 0.9340 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 10. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 11/80\n",
      "2001/2001 [==============================] - 893s 446ms/step - loss: 0.9028 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 11. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 12/80\n",
      "2001/2001 [==============================] - 883s 441ms/step - loss: 0.8731 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 12. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 13/80\n",
      "2001/2001 [==============================] - 908s 454ms/step - loss: 0.8479 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 13. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 14/80\n",
      "2001/2001 [==============================] - 887s 443ms/step - loss: 0.8275 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 14. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 15/80\n",
      "2001/2001 [==============================] - 889s 444ms/step - loss: 0.8096 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 15. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 16/80\n",
      "2001/2001 [==============================] - 883s 441ms/step - loss: 0.7912 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 16. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 17/80\n",
      "2001/2001 [==============================] - 884s 442ms/step - loss: 0.7787 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 17. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 18/80\n",
      "2001/2001 [==============================] - 886s 442ms/step - loss: 0.7575 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 18. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 19/80\n",
      "2001/2001 [==============================] - 892s 446ms/step - loss: 0.7492 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 19. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 20/80\n",
      "2001/2001 [==============================] - 903s 451ms/step - loss: 0.7356 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 20. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 21/80\n",
      "2001/2001 [==============================] - 889s 444ms/step - loss: 0.7219 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 21. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 22/80\n",
      "2001/2001 [==============================] - 884s 442ms/step - loss: 0.7135 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 22. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 23/80\n",
      "2001/2001 [==============================] - 890s 445ms/step - loss: 0.7042 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 23. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 24/80\n",
      "2001/2001 [==============================] - 885s 442ms/step - loss: 0.6959 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 24. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 25/80\n",
      "2001/2001 [==============================] - 903s 451ms/step - loss: 0.6849 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 25. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 26/80\n",
      "2001/2001 [==============================] - 896s 448ms/step - loss: 0.6777 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 26. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 27/80\n",
      "2001/2001 [==============================] - 897s 448ms/step - loss: 0.6651 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 27. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 28/80\n",
      "2001/2001 [==============================] - 887s 443ms/step - loss: 0.6593 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 28. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 29/80\n",
      "2001/2001 [==============================] - 890s 445ms/step - loss: 0.6590 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 29. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 30/80\n",
      "2001/2001 [==============================] - 905s 452ms/step - loss: 0.6500 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 30. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 31/80\n",
      "2001/2001 [==============================] - 891s 445ms/step - loss: 0.6405 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 31. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 32/80\n",
      "2001/2001 [==============================] - 894s 447ms/step - loss: 0.6358 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 32. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 33/80\n",
      "2001/2001 [==============================] - 889s 444ms/step - loss: 0.6281 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 33. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 34/80\n",
      "2001/2001 [==============================] - 888s 444ms/step - loss: 0.6188 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 34. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 35/80\n",
      "2001/2001 [==============================] - 886s 443ms/step - loss: 0.6155 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 35. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 36/80\n",
      "2001/2001 [==============================] - 893s 446ms/step - loss: 0.6160 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 36. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 37/80\n",
      "2001/2001 [==============================] - 904s 452ms/step - loss: 0.6099 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 37. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 38/80\n",
      "2001/2001 [==============================] - 888s 444ms/step - loss: 0.6035 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 38. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 39/80\n",
      "2001/2001 [==============================] - 4004s 2s/step - loss: 0.5981 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 39. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 40/80\n",
      "2001/2001 [==============================] - 883s 441ms/step - loss: 0.5926 - lr: 2.5000e-05\n",
      "\n",
      "Epoch: 40. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      "Epoch 41/80\n",
      " 854/2001 [===========>..................] - ETA: 8:28 - loss: 0.5903"
     ]
    }
   ],
   "source": [
    "\n",
    "# build models\n",
    "generator = Generator(channel=32)\n",
    "\n",
    "\n",
    "# generator_params = generator.parameters()\n",
    "# generator_optimizer = tf.optimizers.Adam(generator_params, opt.lr_gen)\n",
    "\n",
    "\n",
    "image_root = './dataset/train/Imgs/'\n",
    "gt_root = './dataset/train/GT/'\n",
    "fix_root = './dataset/train/Fix/'\n",
    "\n",
    "# train_loader = get_loader(image_root, gt_root, fix_root,batchsize=opt.batchsize, trainsize=opt.trainsize)\n",
    "# total_step = len(train_loader)\n",
    "\n",
    "CE = losses.BinaryCrossentropy(from_logits=True)\n",
    "mse_loss = losses.MeanSquaredError()\n",
    "size_rates = [0.75,1,1.25]  # multi-scale training\n",
    "\n",
    "def structure_loss(pred, mask):\n",
    "    padded = tf.pad(mask,tf.constant([[0,0],[15,15],[15,15],[0,0]]))\n",
    "    pooled =tf.nn.avg_pool2d(padded, ksize=31, strides=1, padding=\"VALID\")\n",
    "    weit  = 1+5*tf.abs(pooled-mask)\n",
    "    weit = tf.squeeze(weit,[3])\n",
    "    wbce= tf.nn.sigmoid_cross_entropy_with_logits(mask,pred)\n",
    "   \n",
    "    wbce= tf.math.reduce_mean(wbce)\n",
    "    wbce  = tf.math.reduce_sum((weit*wbce),axis=[1,2]) /tf.reduce_sum(weit,axis=[1,2])\n",
    "    mask =tf.squeeze(mask,[3])\n",
    "    pred  = tf.math.sigmoid(pred)\n",
    "    pred = tf.squeeze(pred,[3])\n",
    "    inter = tf.math.reduce_sum((pred*mask)*weit,axis=[1,2])\n",
    "    union = tf.math.reduce_sum((pred+mask)*weit, axis=[1,2])\n",
    "    wiou  = 1-(inter+1)/(union-inter+1)\n",
    "    return tf.math.reduce_mean(wbce+wiou)\n",
    "\n",
    "\n",
    "        \n",
    "             \n",
    "def loss_function(y_true,y_pred):\n",
    "    gts, fixs = tf.unstack(y_true,2,0)\n",
    "    gts, _ = tf.split(gts, [1,2], 3)\n",
    "    fixs, _ = tf.split(fixs, [1,2], 3)\n",
    "    fix_pred, cod_pred1, cod_pred2 = tf.unstack(y_pred,num=3,axis=0)\n",
    "    fix_loss = mse_loss(tf.keras.activations.sigmoid(fix_pred),fixs)\n",
    "    cod_loss1 = structure_loss(cod_pred1, gts)\n",
    "    cod_loss2 = structure_loss(cod_pred2, gts)\n",
    "    test= fix_loss + cod_loss1 + cod_loss2\n",
    "    return  fix_loss + cod_loss1 + cod_loss2\n",
    "    \n",
    "def on_epoch_end( epoch, lr):\n",
    "    decay = decay_rate ** (epoch // decay_epoch)\n",
    "    new_lr = lr * decay\n",
    "    print(\"\\nEpoch: {}. Reducing Learning Rate from {} to {}\".format(epoch, lr, new_lr))\n",
    "    return new_lr\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch='10, 15')\n",
    "    \n",
    "    op= tf.keras.optimizers.Adam(learning_rate=2.5e-5, name='Adam')\n",
    "    \n",
    "    generator.compile(optimizer=op, loss=loss_function, run_eagerly=False)\n",
    "    \n",
    "    \n",
    "    data = get_loader(image_root, gt_root, fix_root, 480, batchsize, size_rates)\n",
    "    print(len(data))\n",
    "    generator.fit(x=data,batch_size=batchsize, epochs=epoch, verbose='auto' , callbacks=[tensorboard_callback, tf.keras.callbacks.LearningRateScheduler(on_epoch_end)])\n",
    "     \n",
    "     \n",
    "    save_path = 'models/Resnet/'\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dataset_path = './dataset/test/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "print(\"Generator is build\")\n",
    "generator.save(\"results/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_datasets = ['Mine', 'CAMO']\n",
    "for dataset in test_datasets:\n",
    "    save_path = './results/ResNet50/' + dataset + '/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    image_root = dataset_path + dataset + '/Imgs/'\n",
    "    test_loader = test_dataset(image_root, 480)\n",
    "\n",
    "    for i in range(test_loader.size):\n",
    "        print(i)\n",
    "        image, HH, WW, name = test_loader.load_data()\n",
    "        ans = generator(image)\n",
    "        _,generator_pred, _  = tf.unstack(ans,num=3,axis=0)\n",
    "        res = generator_pred\n",
    "        res = tf.image.resize(res, size=tf.constant([WW,HH]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        res = tf.math.sigmoid(res).numpy().squeeze()\n",
    "        res = 255*(res - res.min()) / (res.max() - res.min() + 1e-8)\n",
    "        print(save_path+name)\n",
    "        cv2.imwrite(save_path+name, res)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4575b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#generator2 = Generator(channel=32)\n",
    "generator2 =  tf.keras.models.load_model('./results/model', custom_objects={'loss_function': loss_function})\n",
    "for dataset in test_datasets:\n",
    "    save_path = './results/small/' + dataset + '/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    image_root = dataset_path + dataset + '/Imgs/'\n",
    "    test_loader = test_dataset(image_root, 480)\n",
    "\n",
    "    for i in range(test_loader.size):\n",
    "        print(i)\n",
    "        image, HH, WW, name = test_loader.load_data()\n",
    "        ans = generator2(image)\n",
    "        _,generator_pred, _  = tf.unstack(ans,num=3,axis=0)\n",
    "        res = generator_pred\n",
    "        res = tf.image.resize(res, size=tf.constant([WW,HH]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        res = tf.math.sigmoid(res).numpy().squeeze()\n",
    "        res = 255*(res - res.min()) / (res.max() - res.min() + 1e-8)\n",
    "        print(save_path+name)\n",
    "        cv2.imwrite(save_path+name, res)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb00c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
