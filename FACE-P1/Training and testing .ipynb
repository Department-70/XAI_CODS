{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5e7f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Quadro RTX 3000, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Feb 17 13:11:12 2023\n",
    "\n",
    "@author: Debra Hogue\n",
    "\n",
    "Modified RankNet by Lv et al. to use Tensorflow not Pytorch\n",
    "and added additional comments to explain methods\n",
    "\n",
    "Paper: Simultaneously Localize, Segment and Rank the Camouflaged Objects by Lv et al.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, layers, losses\n",
    "import numpy as np\n",
    "import os, argparse\n",
    "from datetime import datetime\n",
    "from Attention.ResNet_models import Generator\n",
    "from data import get_loader\n",
    "from utils import adjust_lr, AvgMeter\n",
    "from scipy import misc\n",
    "import cv2\n",
    "from data import test_dataset\n",
    "from PIL import ImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2\n",
    "import tensorflow.keras.applications.resnet50 as models # instantiates the ResNet50 architecture \n",
    "\n",
    "from utils import l2_regularisation\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=5632)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "tf.config.optimizer.set_experimental_options({\"Memory optimizer\": True, \"Layout optimizer\": True})\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203e9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--epoch', type=int, default=1, help='epoch number')\n",
    "# parser.add_argument('--lr_gen', type=float, default=2.5e-5, help='learning rate for generator')\n",
    "# parser.add_argument('--batchsize', type=int, default=2, help='training batch size')\n",
    "# parser.add_argument('--trainsize', type=int, default=480, help='training dataset size')\n",
    "# parser.add_argument('--decay_rate', type=float, default=0.9, help='decay rate of learning rate')\n",
    "# parser.add_argument('--decay_epoch', type=int, default=40, help='every n epochs decay learning rate')\n",
    "# parser.add_argument('--feat_channel', type=int, default=32, help='reduced channel of saliency feat')\n",
    "# opt = parser.parse_args()\n",
    "# print('Generator Learning Rate: {}'.format(opt.lr_gen))\n",
    "epoch = 1\n",
    "decay_rate = 0.97\n",
    "decay_epoch = 65\n",
    "batchsize = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8744bfe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# build models\n",
    "generator = Generator(channel=32)\n",
    "\n",
    "\n",
    "# generator_params = generator.parameters()\n",
    "# generator_optimizer = tf.optimizers.Adam(generator_params, opt.lr_gen)\n",
    "\n",
    "\n",
    "image_root = 'dataset/train/Imgs/'\n",
    "gt_root = 'dataset/train/GT/'\n",
    "fix_root = 'dataset/train/Fix/'\n",
    "\n",
    "# train_loader = get_loader(image_root, gt_root, fix_root,batchsize=opt.batchsize, trainsize=opt.trainsize)\n",
    "# total_step = len(train_loader)\n",
    "\n",
    "CE = losses.BinaryCrossentropy(from_logits=True)\n",
    "mse_loss = losses.MeanSquaredError()\n",
    "size_rates = [0.75,1,1.25]  # multi-scale training\n",
    "\n",
    "def structure_loss(pred, mask):\n",
    "    padded = tf.pad(mask,tf.constant([[0,0],[15,15],[15,15],[0,0]]))\n",
    "    pooled =tf.nn.avg_pool2d(padded, ksize=31, strides=1, padding=\"VALID\")\n",
    "    weit  = 1+5*tf.abs(pooled-mask)\n",
    "    weit = tf.squeeze(weit,[3])\n",
    "    wbce= tf.nn.sigmoid_cross_entropy_with_logits(mask,pred)\n",
    "   \n",
    "    wbce= tf.math.reduce_mean(wbce)\n",
    "    wbce  = tf.math.reduce_sum((weit*wbce),axis=[1,2]) /tf.reduce_sum(weit,axis=[1,2])\n",
    "    mask =tf.squeeze(mask,[3])\n",
    "    pred  = tf.math.sigmoid(pred)\n",
    "    pred = tf.squeeze(pred,[3])\n",
    "    inter = tf.math.reduce_sum((pred*mask)*weit,axis=[1,2])\n",
    "    union = tf.math.reduce_sum((pred+mask)*weit, axis=[1,2])\n",
    "    wiou  = 1-(inter+1)/(union-inter+1)\n",
    "    return tf.math.reduce_mean(wbce+wiou)\n",
    "\n",
    "\n",
    "        \n",
    "             \n",
    "def loss_function(y_true,y_pred):\n",
    "    gts, fixs = tf.unstack(y_true,2,0)\n",
    "    gts, _ = tf.split(gts, [1,2], 3)\n",
    "    fixs, _ = tf.split(fixs, [1,2], 3)\n",
    "    fix_pred, cod_pred1, cod_pred2 = tf.unstack(y_pred,num=3,axis=0)\n",
    "    tf.summary.image(\"fixation\",fix_pred)\n",
    "    fix_loss = mse_loss(tf.keras.activations.sigmoid(fix_pred),fixs)\n",
    "    cod_loss1 = structure_loss(cod_pred1, gts)\n",
    "    cod_loss2 = structure_loss(cod_pred2, gts)\n",
    "    fix_loss_scaler = tf.squeeze(fix_loss)\n",
    "    tf.summary.scalar(\"fix_loss\",fix_loss)\n",
    "    test= fix_loss + cod_loss1 + cod_loss2\n",
    "    return  test \n",
    "    \n",
    "def on_epoch_end( epoch, lr):\n",
    "    decay = decay_rate ** (epoch // decay_epoch)\n",
    "    new_lr = lr * decay\n",
    "    print(\"\\nEpoch: {}. Reducing Learning Rate from {} to {}\".format(epoch, lr, new_lr))\n",
    "    return new_lr\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fb99a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data = get_loader(image_root, gt_root, fix_root, 480, batchsize, size_rates)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "mse_loss = losses.MeanSquaredError()\n",
    "#for i in data:\n",
    " #   img, label = i\n",
    "  #  gts, fixs = tf.unstack(label,2,0)\n",
    "  #  gts, _ = tf.split(gts, [1,2], 3)\n",
    "  #  fixs, _ = tf.split(fixs, [1,2], 3)\n",
    "    \n",
    "    \n",
    "    #res2 = fixs\n",
    "    #res2 = tf.image.resize(res2, size=tf.constant([WW,HH]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "    #res2 = tf.math.sigmoid(res2).numpy().squeeze()\n",
    "    #es2 = (res2 - res2.min()) / (res.max() - res2.min() + 1e-8)\n",
    "fix1 = cv2.imread('dataset/train/train/COD10K-CAM-1-Aquatic-13-Pipefish-634.png')   \n",
    "fixGT = cv2.imread('dataset/train/Fix/COD10K-CAM-1-Aquatic-13-Pipefish-634.png') \n",
    "\n",
    "  \n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(fixGT)\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(fix1)\n",
    "\n",
    "fix1= fix1.astype(float)\n",
    "fixGT=fixGT.astype(float)\n",
    "\n",
    "fix1 = tf.constant(fix1)\n",
    "fixGT = tf.constant(fixGT)\n",
    "fix1=tf.math.sigmoid(fix1)\n",
    "fixGT=tf.math.sigmoid(fixGT) \n",
    "diff = fix1-fixGT\n",
    "print(diff.shape)\n",
    "loss = tf.math.reduce_mean(tf.math.square(diff))\n",
    "print(loss)\n",
    "loss = mse_loss(fixGT,fix1)\n",
    "print(loss)\n",
    "print(fix1)\n",
    "print(fixGT)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471a068",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "\n",
      "Epoch: 0. Reducing Learning Rate from 2.499999936844688e-05 to 2.499999936844688e-05\n",
      " 575/2001 [=======>......................] - ETA: 39:46 - loss: 190.9509"
     ]
    }
   ],
   "source": [
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch='10, 15',write_images=True)\n",
    "    \n",
    "    op= tf.keras.optimizers.Adam(learning_rate=2.5e-5, name='Adam')\n",
    "    \n",
    "    generator.compile(optimizer=op, loss=loss_function, run_eagerly=False)\n",
    "    \n",
    "    \n",
    "    data = get_loader(image_root, gt_root, fix_root, 480, batchsize, size_rates)\n",
    "    print(len(data))\n",
    "    generator.fit(x=data,batch_size=batchsize, epochs=epoch, verbose='auto' , callbacks=[tensorboard_callback, tf.keras.callbacks.LearningRateScheduler(on_epoch_end)])\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path = 'models/Resnet/'\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_path = 'dataset/Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "print(\"Generator is build\")\n",
    "generator.save(\"results/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057edf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = ['Mine', 'CAMO']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset in test_datasets:\n",
    "    save_path = './results/ResNet50/' + dataset + '/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    image_root = dataset_path + dataset + '/Imgs/'\n",
    "    test_loader = test_dataset(image_root, 480)\n",
    "\n",
    "    for i in range(test_loader.size):\n",
    "        print(i)\n",
    "        image, HH, WW, name = test_loader.load_data()\n",
    "        ans = generator(image)\n",
    "        _,generator_pred, _  = tf.unstack(ans,num=3,axis=0)\n",
    "        res = generator_pred\n",
    "        res = tf.image.resize(res, size=tf.constant([WW,HH]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        res = tf.math.sigmoid(res).numpy().squeeze()\n",
    "        res = 255*(res - res.min()) / (res.max() - res.min() + 1e-8)\n",
    "        print(save_path+name)\n",
    "        cv2.imwrite(save_path+name, res)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ebcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator2 =  tf.keras.models.load_model('./models/FACE-100/', custom_objects={'loss_function': loss_function})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4575b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset in test_datasets:\n",
    "    save_path = './results/small/' + dataset + '/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    image_root = dataset_path + dataset + '/Imgs/'\n",
    "    test_loader = test_dataset(image_root, 480)\n",
    "\n",
    "    for i in range(test_loader.size):\n",
    "        print(i)\n",
    "        image, HH, WW, name = test_loader.load_data()\n",
    "        ans = generator2(image)\n",
    "        generator_pred,_, _  = tf.unstack(ans,num=3,axis=0)\n",
    "        print(generator_pred)\n",
    "        res = generator_pred\n",
    "        res = tf.image.resize(res, size=tf.constant([WW,HH]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        res = tf.math.sigmoid(res).numpy().squeeze()\n",
    "        res = 255*(res - res.min()) / (res.max() - res.min() + 1e-8)\n",
    "        print(save_path+name)\n",
    "        cv2.imwrite(save_path+name, res)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb00c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
